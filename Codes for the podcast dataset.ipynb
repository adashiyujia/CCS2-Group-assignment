{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ff4139-1a9c-4850-8470-b930e70aa954",
   "metadata": {},
   "source": [
    "# Group assignment (Podcast dataset)\n",
    "#### Minor: Communication in the Digital Society\n",
    "#### Course: CCS 2\n",
    "#### Tutorial group 1\n",
    "#### Tutorial teacher: Isa van Leeuwen\n",
    "#### Group members: Ada Shi (13558846), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6341e-f181-4da0-9e25-a3acf8224e73",
   "metadata": {},
   "source": [
    "## Code for exploring data analysis\n",
    "#### In this part, we will explore the dataset by:\n",
    "* checking all columns and their datatypes and checking for missing values\n",
    "* following pre-processing steps that are learned in the first week of this course (eg. lowercasing, tokenization, stop words removal/pruning, lemmatization, and N-grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90225793-5968-479d-b987-cf08afb8dd09",
   "metadata": {},
   "source": [
    "#### Data exploration (learned in CCS 1):\n",
    "##### (this part is not important for this assignment but good for personal understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d38201-4554-4741-a7da-234c9cb625f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages & the podcast dataset to the jupyter notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import spacy\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "df_podcast = pd.read_csv(\"poddf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe7bbcf-c7a1-460d-b707-c362b101a541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Name', 'Rating_Volume', 'Rating', 'Genre', 'Description'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check columns\n",
    "\n",
    "df_podcast.columns # there are 6 columns in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ccfff9-56ee-400a-9451-38b3849d2cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index             int64\n",
       "Name             object\n",
       "Rating_Volume    object\n",
       "Rating           object\n",
       "Genre            object\n",
       "Description      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes for all columns\n",
    "\n",
    "df_podcast.dtypes # except for the column \"index\", the datatypes of the rest of the columns are all object \n",
    "\n",
    "#the datatypes of columns \"Rating_Volume\" and \"Rating\" are wrong (\"Rating_Volume\" should be int64 and \"Rating\" should be float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01972d-7e60-4e66-a53a-940325194d4e",
   "metadata": {},
   "source": [
    "##### Explanations\n",
    "\n",
    "##### Modification on \"Rating_Volume\" column\n",
    "* We wanted to convert the datatype of \"Rating_Volume\" to int64 via the code as shown below:\n",
    "* df_podcast[\"Rating_Volume\"] = df_podcast[\"Rating_Volume\"].astype(int)\n",
    "* However, it produced an error \"ValueError: invalid literal for int() with base 10: 'Not Found'\"\n",
    "\n",
    "##### Modification on \"Rating_Volume\" column\n",
    "* Similarly, we wanted to convert the datatype of \"Rating\" to float64 via the code as shown below:\n",
    "* df_podcast[\"Rating\"] = df_podcast[\"Rating\"].astype(float)\n",
    "* the output showed an error \"ValueError: could not convert string to float: 'Not Found'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df215a8-9ea2-4c4f-b600-a581911b52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction\n",
    "\n",
    "## Replace \"Not Found\" values with NaN\n",
    "df_podcast[\"Rating_Volume\"] = df_podcast[\"Rating_Volume\"].replace(\"Not Found\", np.nan)\n",
    "df_podcast[\"Rating\"] = df_podcast[\"Rating\"].replace(\"Not Found\", np.nan)\n",
    "\n",
    "## Change datatypes\n",
    "df_podcast[\"Rating\"] = df_podcast[\"Rating\"].astype(float)\n",
    "df_podcast[\"Rating_Volume\"] = df_podcast[\"Rating_Volume\"].astype(\"Int64\") # convert NaN to nullable integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dff395-9333-458b-9f96-3ef015a224ac",
   "metadata": {},
   "source": [
    "##### Explanation for the code that converts NaN to nullable integer\n",
    "* After replacing \"Not Found\" with NaN, we wanted to convert \"Rating_Volume\" to int64 via the code below:\n",
    "* df_podcast[\"Rating_Volume\"] = df_podcast[\"Rating_Volume\"].astype(int)\n",
    "* However, it produced an error \"ValueError: cannot convert NA to integer\" because NaN is a float\n",
    "* To solve this problem, instead of trying to convert all values under the column \"Rating_Volume\" to int64, we converted them to nullable integer type (\"Int64\") - a datatype that allows the storage of both regular integers and missing values (need to find literature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7beb58b2-d71a-4d2f-b501-05123385cc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index              int64\n",
       "Name              object\n",
       "Rating_Volume      Int64\n",
       "Rating           float64\n",
       "Genre             object\n",
       "Description       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_podcast.dtypes # now the datatypes changed to [Rating_Volume] - \"Int64\" and [Rating] - \"float64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc9e377b-1751-4864-81c1-7aecd6b9bbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index               0\n",
       "Name                0\n",
       "Rating_Volume    1887\n",
       "Rating           1887\n",
       "Genre               0\n",
       "Description         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "\n",
    "df_podcast.isna().sum() # there are 1887 missing values under the columns \"Rating_Volume\" and \"Rating\" (because there are NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e4f42-082f-4c32-a02a-a4d3c5eb67b5",
   "metadata": {},
   "source": [
    "#### Specification:\n",
    "* For this assignment, we are most interested in the last objective column \"Description\"\n",
    "* to develop our own recommender system, we will first, transform this column into a list and then pre-process the data with knowledge learned from week 1 (eg. lowercasing, stopwords removal, lemmatization/stemming, tokenization, etc.)\n",
    "* second, we will adopt a bottom-up (inductive) approach for the text analysis that is learned from week 3 (eg. CountVectorizer/TfidVectorizer, Cosine similarity/Soft-cosine similarity, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8ad91-4915-4551-8450-5c2d7f199a49",
   "metadata": {},
   "source": [
    "#### 1. Pre-processing (learned in CCS 2, week 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab3f3bdc-23c8-421a-9f7a-26e837a21851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text-column into a list\n",
    "\n",
    "text_list = df_podcast[\"Description\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42956198-305c-415f-b96a-03da2af4a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing\n",
    "\n",
    "text_list_lower = [text.lower() for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc099ec-5f21-4aa7-96e3-da58f2937b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "\n",
    "mystopwords = stopwords.words(\"english\")\n",
    "text_without_stopwords = [\" \".join([w for w in text.split() if w not in mystopwords]) for text in text_list_lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e90463-7b47-40b3-8cda-4f9fe8c2130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatized_text = [\" \".join([w.lemma_ for w in nlp(text)]) for text in text_without_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "732bc271-653b-4cfd-9f00-fe42a8199703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens removed punctuations\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text_without_punctuations = [tokenizer.tokenize(text) for text in lemmatized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f08ec17-714f-449d-b31a-54387f8b6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"s\"\n",
    "\n",
    "tokens = [([w for w in text if w != \"s\"]) for text in text_without_punctuations]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c04ec7-1d0c-4457-b1fb-f6286a33fb1d",
   "metadata": {},
   "source": [
    "#### Specification:\n",
    "* The pre-processing follows the order as shown above because we want to keep standard tokens without any punctuations\n",
    "* after lowercasing, we remove stopwords that are not informative\n",
    "* and then the lemmatization transforms each token to word can be found in dictionaries\n",
    "* after lemmatizing, some tokens are in the form of \"contraction expansion eg. it's -> it 's\", so we seperate the pruning process into 2 steps (removing stopwords and removing punctuations) and remove punctuations after lemmatization\n",
    "* in the final pre-processing step,we removed a particular token \"s\" because we observed many \"s\" in the output while inspecting previous steps, this letter left after \"contraction expansion\" is not informative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed225799-d69b-4a2e-82c5-612a6c93e23c",
   "metadata": {},
   "source": [
    "#### 2. Inductive analysis (learned in CCS 2, week 3):\n",
    "#### Part 1: Vectorization\n",
    "* In this part, we will use the Tfidfvectorizer in a sparse format\n",
    "* the aim is to analyze the data from the bottom of texts up to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500c2b1-f169-4d79-91c1-c70e23d0640a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d537d84-67aa-4ea2-82fa-46676a490b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
